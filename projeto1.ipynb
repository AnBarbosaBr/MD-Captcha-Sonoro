{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importings base libraries\n",
    "import os;\n",
    "import pandas as pd; \n",
    "import numpy as np;\n",
    "import librosa;\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Models\n",
    "import sklearn.model_selection # train_test_split\n",
    "import sklearn.discriminant_analysis # LinearDiscriminantAnalysis\n",
    "import sklearn.naive_bayes  # GaussianNB\n",
    "\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ingestion Functions\n",
    "def _process_wave_file(wave_file, labels_list, filename_list, duration_list, sr_list, data_list, interval_time = 2):\n",
    "    ''' This function will append to the lists with the data from the wave file. \n",
    "    It does not have a return'''\n",
    "    # Get data from the wave file:\n",
    "    audio_data, sampling_rate = librosa.load(wave_file, None)\n",
    "    original_filename = os.path.basename(wave_file)\n",
    "    original_filename = os.path.splitext(original_filename)[0]\n",
    "\n",
    "    # Calculate Some Attributes\n",
    "    labels = list(original_filename)[0:4] # each label contains 4 letters\n",
    "    frames_per_audio = sampling_rate * interval_time\n",
    "    \n",
    "    \n",
    "    # Separate the Wave File in interval_time sections.\n",
    "    rows_processed = 0\n",
    "    \n",
    "    for i, ini in enumerate(range(0, audio_data.shape[0], frames_per_audio)):\n",
    "            \n",
    "            # Calculate attributes\n",
    "            this_audio = pd.Series(audio_data[ini:(ini+frames_per_audio)])\n",
    "            this_duration = this_audio.shape[0]/sampling_rate\n",
    "            # Update the lists with this section data.\n",
    "            rows_processed += 1\n",
    "            filename_list.append(original_filename)\n",
    "            duration_list.append(this_duration)\n",
    "            sr_list.append(sampling_rate)\n",
    "            data_list.append(this_audio)\n",
    "            \n",
    "            \n",
    "   \n",
    "    # If we process more intervals than those predicted by our original_filename,\n",
    "    # We label as \"?\"\n",
    "    while(len(labels) < rows_processed):\n",
    "        #print(f\"adding ? to {original_filename}\")\n",
    "        labels.append(\"?\")  \n",
    "    \n",
    "    # Update the labels list.\n",
    "    labels_list.extend(labels)\n",
    "\n",
    "def _load_wavs_from_dir(directory, verbose=False):\n",
    "    # Using those imports only on this function\n",
    "    from os.path import isfile, join\n",
    "    from os import listdir\n",
    "    \n",
    "    # Reading wave files from the directory\n",
    "    wave_files = [join(directory , f) for f in listdir(directory) if (isfile(join(directory, f)) and f.endswith(\".wav\")) ]\n",
    "    \n",
    "    # Creating lists that will store the data\n",
    "    labels_list = list()\n",
    "    filename_list = list() \n",
    "    duration_list = list()\n",
    "    sr_list = list() \n",
    "    data_list = list()\n",
    "    \n",
    "    # Auxiliar variables\n",
    "    processed = 1;   # For Verbose output\n",
    "    to_be_processed = len(wave_files) # For Verbose output\n",
    "    \n",
    "    for file in wave_files:\n",
    "        if(verbose): print(f\"{file}: Processing {processed} of {to_be_processed}.\")\n",
    "        _process_wave_file(file, labels_list, filename_list, duration_list, sr_list, data_list)\n",
    "        processed += 1\n",
    "    # After process all the files, create the DataFrame\n",
    "    if(verbose): print(\"Creating DataFrame\")\n",
    "    df = pd.DataFrame(data_list)\n",
    "    if(verbose): print(\"Inserting Labels...\")\n",
    "    df.insert(loc  = 0, column = 'label', value = labels_list)\n",
    "    if(verbose): print(\"Inserting Duração...\")\n",
    "    df.insert(loc  = 1, column = 'duracao', value = duration_list)\n",
    "    if(verbose): print(\"Inserting Sampling Rates(sr)...\")\n",
    "    df.insert(loc = 2, column = 'sr', value = sr_list )\n",
    "    if(verbose): print(\"Inserting Original Filename...\")\n",
    "    df.insert(loc = 3, column = \"original_file\", value = filename_list)\n",
    "    if(verbose): print(\"DataFrame Created. Returning\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline Functions \n",
    "def load_data(data_directory, output_pickle_file = None, reuse_if_exists=True):\n",
    "    if(output_pickle_file):\n",
    "        output_extension = os.path.splitext(output_pickle_file)[1]\n",
    "        if (output_extension != \".pickle\"):\n",
    "            raise(\"Output must be a file ended with .pickle\") \n",
    "    \n",
    "        if( reuse_if_exists and os.path.isfile(output_pickle_file) ):\n",
    "            # If the user wants to reuse existing pickle file and it exists\n",
    "            return pd.read_pickle(output_pickle_file)\n",
    "        \n",
    "        else:\n",
    "            # If the user do not wan´t to use existing file, or if it does not exists\n",
    "            df = _load_wavs_from_dir(data_directory)\n",
    "            df.to_pickle(output_pickle_file)\n",
    "            return df\n",
    "    return(_load_wavs_from_dir(data_directory))\n",
    "\n",
    "def preprocess_data(df):\n",
    "    ''' Filtre, remova nulls, e transforme os dados nessa etapa'''\n",
    "    return df.fillna(0, inplace=False)\n",
    "\n",
    "def extract_features(df):\n",
    "    features = df.iloc[ : , df.columns.get_loc(0): ]\n",
    "    return features\n",
    "\n",
    "def extract_labels(df):\n",
    "    labels = df.loc[ : , \"label\"]\n",
    "    \n",
    "    label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main functions\n",
    "def process_data(training_data, validation_data, algorithm):\n",
    "    # Preprocess - Filter and Imputing \n",
    "    train_data = preprocess_data(training_data)\n",
    "    test_data  = preprocess_data(validation_data)\n",
    "    \n",
    "    # Extracting information\n",
    "    x_train = extract_features(train_data)\n",
    "    y_train = extract_labels(train_data)\n",
    "    \n",
    "    x_test = extract_features(test_data)\n",
    "    y_test = extract_labels(test_data)\n",
    "    \n",
    "    # Fit model\n",
    "    algorithm.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    predict_train = algorithm.predict(x_train)\n",
    "    predict_test  = algorithm.predict(x_test)\n",
    "    \n",
    "    return(predict_train, predict_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def process_folder(training_folder, validation_folder, algorithm):\n",
    "    ## Ler os dados\n",
    "    training_data = load_data(training_folder)\n",
    "    validation_data = load_data(validation_folder)\n",
    "    \n",
    "    return process_data(training_data, validation_data, algorithm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING THE MODEL\n",
    "## Inputs\n",
    "train_path = \".\\\\dados\\\\treinar\\\\\"\n",
    "test_path  = \".\\\\dados\\\\validar\\\\\"\n",
    "\n",
    "## Input Saving -> Will be used to avoid having to reload all data\n",
    "train_pickle = \".\\\\dados\\\\saving_treinamento.pickle\"\n",
    "test_pickle = \".\\\\dados\\\\saving_teste.pickle\"\n",
    "\n",
    "## Algorithms\n",
    "lda = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "lda2 = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Testando ler os dados\n",
    "training_data = load_data(train_path, train_pickle)\n",
    "validation_data = load_data(test_path, test_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program_files\\Miniconda3\\envs\\md\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "train_predict, test_predict = process_data(training_data, validation_data, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program_files\\Miniconda3\\envs\\md\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Testando ler os diretorios\n",
    "\n",
    "\n",
    "train_predict_from_folder, test_predict_from_folder = process_folder(train_path, test_path, lda2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
