{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importings base libraries\n",
    "import os;\n",
    "import pandas as pd; \n",
    "import numpy as np;\n",
    "import librosa;\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Models\n",
    "import sklearn.model_selection # train_test_split\n",
    "import sklearn.discriminant_analysis # LinearDiscriminantAnalysis\n",
    "import sklearn.naive_bayes  # GaussianNB\n",
    "\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ingestion Functions\n",
    "def _process_wave_file(wave_file, labels_list, filename_list, duration_list, sr_list, data_list, interval_time = 2):\n",
    "    ''' This function will append to the lists with the data from the wave file. \n",
    "    It does not have a return'''\n",
    "    # Get data from the wave file:\n",
    "    audio_data, sampling_rate = librosa.load(wave_file, None)\n",
    "    original_filename = os.path.basename(wave_file)\n",
    "    original_filename = os.path.splitext(original_filename)[0]\n",
    "\n",
    "    # Calculate Some Attributes\n",
    "    labels = list(original_filename)[0:4] # each label contains 4 letters\n",
    "    frames_per_audio = sampling_rate * interval_time\n",
    "    \n",
    "    \n",
    "    # Separate the Wave File in interval_time sections.\n",
    "    rows_processed = 0\n",
    "    \n",
    "    for i, ini in enumerate(range(0, audio_data.shape[0], frames_per_audio)):\n",
    "            \n",
    "            # Calculate attributes\n",
    "            this_audio = pd.Series(audio_data[ini:(ini+frames_per_audio)])\n",
    "            this_duration = this_audio.shape[0]/sampling_rate\n",
    "            # Update the lists with this section data.\n",
    "            rows_processed += 1\n",
    "            filename_list.append(original_filename)\n",
    "            duration_list.append(this_duration)\n",
    "            sr_list.append(sampling_rate)\n",
    "            data_list.append(this_audio)\n",
    "            \n",
    "            \n",
    "   \n",
    "    # If we process more intervals than those predicted by our original_filename,\n",
    "    # We label as \"?\"\n",
    "    while(len(labels) < rows_processed):\n",
    "        #print(f\"adding ? to {original_filename}\")\n",
    "        labels.append(\"?\")  \n",
    "    \n",
    "    # Update the labels list.\n",
    "    labels_list.extend(labels)\n",
    "\n",
    "def _load_wavs_from_dir(directory, verbose=False):\n",
    "    # Using those imports only on this function\n",
    "    from os.path import isfile, join\n",
    "    from os import listdir\n",
    "    \n",
    "    # Reading wave files from the directory\n",
    "    wave_files = [join(directory , f) for f in listdir(directory) if (isfile(join(directory, f)) and f.endswith(\".wav\")) ]\n",
    "    \n",
    "    # Creating lists that will store the data\n",
    "    labels_list = list()\n",
    "    filename_list = list() \n",
    "    duration_list = list()\n",
    "    sr_list = list() \n",
    "    data_list = list()\n",
    "    \n",
    "    # Auxiliar variables\n",
    "    processed = 1;   # For Verbose output\n",
    "    to_be_processed = len(wave_files) # For Verbose output\n",
    "    \n",
    "    for file in wave_files:\n",
    "        if(verbose): print(f\"{file}: Processing {processed} of {to_be_processed}.\")\n",
    "        _process_wave_file(file, labels_list, filename_list, duration_list, sr_list, data_list)\n",
    "        processed += 1\n",
    "    # After process all the files, create the DataFrame\n",
    "    if(verbose): print(\"Creating DataFrame\")\n",
    "    df = pd.DataFrame(data_list)\n",
    "    if(verbose): print(\"Inserting Labels...\")\n",
    "    df.insert(loc  = 0, column = 'label', value = labels_list)\n",
    "    if(verbose): print(\"Inserting Duração...\")\n",
    "    df.insert(loc  = 1, column = 'duracao', value = duration_list)\n",
    "    if(verbose): print(\"Inserting Sampling Rates(sr)...\")\n",
    "    df.insert(loc = 2, column = 'sr', value = sr_list )\n",
    "    if(verbose): print(\"Inserting Original Filename...\")\n",
    "    df.insert(loc = 3, column = \"original_file\", value = filename_list)\n",
    "    if(verbose): print(\"DataFrame Created. Returning\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline Functions \n",
    "def load_data(data_directory, output_pickle_file = None, reuse_if_exists=True):\n",
    "    if(output_pickle_file):\n",
    "        output_extension = os.path.splitext(output_pickle_file)[1]\n",
    "        if (output_extension != \".pickle\"):\n",
    "            raise(\"Output must be a file ended with .pickle\") \n",
    "    \n",
    "        if( reuse_if_exists and os.path.isfile(output_pickle_file) ):\n",
    "            # If the user wants to reuse existing pickle file and it exists\n",
    "            return pd.read_pickle(output_pickle_file)\n",
    "        \n",
    "        else:\n",
    "            # If the user do not wan´t to use existing file, or if it does not exists\n",
    "            df = _load_wavs_from_dir(data_directory)\n",
    "            df.to_pickle(output_pickle_file)\n",
    "            return df\n",
    "    return(_load_wavs_from_dir(data_directory))\n",
    "\n",
    "def preprocess_data(df):\n",
    "    ''' Filtre, remova nulls, e transforme os dados nessa etapa'''\n",
    "    prepared = df[df.label != \"?\"] # Remove rows with unknown labels\n",
    "    return prepared.fillna(0, inplace=False)\n",
    "\n",
    "def extract_features(df):\n",
    "    features = df.iloc[ : , df.columns.get_loc(0): ]\n",
    "    return features\n",
    "\n",
    "def extract_labels(df):\n",
    "    labels = df.loc[ : , \"label\"]\n",
    "    \n",
    "    label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    return labels\n",
    "\n",
    "def score_classifier(df, y_pred):\n",
    "    prepared_data = preprocess_data(df)\n",
    "    y_real = extract_labels(prepared_data)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(sklearn.metrics.confusion_matrix(y_true=y_real, y_pred = y_pred))\n",
    "    \n",
    "    print(\"\\n Other Observations:\")\n",
    "    \n",
    "    trues = y_real == y_pred\n",
    "    hits = sum(trues)\n",
    "    total = len(y_pred)\n",
    "    print(f\"It got right: {hits} from {total} letters: {100*hits/total :.2f}%\")\n",
    "    word_hits = prepared_data[trues].original_file.value_counts()\n",
    "    unique_words = len(df.original_file.unique())\n",
    "    print(f\"It received {unique_words} words. \")\n",
    "    print(f\"It got right 4 letters of: {sum(word_hits == 4)} words.\\n\" +\n",
    "          f\"It got right 3 letters of: {sum(word_hits == 3)} words.\\n\" +\n",
    "          f\"It got right 2 letters of: {sum(word_hits == 2)} words.\\n\" +\n",
    "          f\"It got right 1 letters of: {sum(word_hits == 1)} words.\\n\" +\n",
    "          f\"It got right 0 letters of: {unique_words - len(word_hits)} words.\\n\")\n",
    "    \n",
    "    print(\"Those are the words and hit count:\")\n",
    "    print(word_hits)\n",
    "    print(\"Those are the letters:\")\n",
    "    print(prepared_data[trues].label.value_counts())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main functions\n",
    "def process_data(training_data, validation_data, algorithm):\n",
    "    # Preprocess - Filter and Imputing \n",
    "    train_data = preprocess_data(training_data)\n",
    "    test_data  = preprocess_data(validation_data)\n",
    "    \n",
    "    # Extracting information\n",
    "    x_train = extract_features(train_data)\n",
    "    y_train = extract_labels(train_data)\n",
    "    \n",
    "    x_test = extract_features(test_data)\n",
    "    y_test = extract_labels(test_data)\n",
    "    \n",
    "    # Fit model\n",
    "    algorithm.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    predict_train = algorithm.predict(x_train)\n",
    "    predict_test  = algorithm.predict(x_test)\n",
    "    \n",
    "    score_classifier(validation_data, predict_test)\n",
    "    \n",
    "    return(predict_train, predict_test)\n",
    "    \n",
    "def process_folder(training_folder, validation_folder, algorithm):\n",
    "    ## Ler os dados\n",
    "    training_data = load_data(training_folder, verbose=True)\n",
    "    validation_data = load_data(validation_folder, verbose=True)\n",
    "    \n",
    "    return process_data(training_data, validation_data, algorithm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING THE MODEL\n",
    "## Inputs\n",
    "### In-paths\n",
    "train_path = \".\\\\dados\\\\TREINAMENTO\\\\\"\n",
    "test_path  = \".\\\\dados\\\\VALIDACAO\\\\\"\n",
    "\n",
    "### Out-paths -> Will be used to avoid having to reload all data\n",
    "train_pickle = \".\\\\dados\\\\treina_1752.pickle\"\n",
    "test_pickle = \".\\\\dados\\\\valida_1752.pickle\"\n",
    "\n",
    "## Algorithms\n",
    "lda = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "nb = sklearn.naive_bayes.GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Loading\n",
    "training_data = load_data(train_path, train_pickle)\n",
    "validation_data = load_data(test_path, test_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program_files\\Miniconda3\\envs\\md\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[11  8  5  9 13 13  0 11 13 17]\n",
      " [ 9 16  9  6 19  8  5 15  9 18]\n",
      " [ 9  7  3  8 22 20  5  9  5  9]\n",
      " [ 9  4  5  8 19 18  1 17 12 13]\n",
      " [14  5  8 15 21 10  3  8 11 12]\n",
      " [11  2  7 12 21 17  1 12 13 14]\n",
      " [13  8  9 12 14 22  7 15  7 15]\n",
      " [ 9  7 10 12 15 15  2  7 12 15]\n",
      " [ 9  6  5 15 19 16  6 15 10 12]\n",
      " [ 7  3 10  6 14 14  2 12 10 17]]\n",
      "\n",
      " Other Observations:\n",
      "It got right: 117 from 1068 letters: 10.96%\n",
      "It received 267 words. \n",
      "It got right 4 letters of: 0 words.\n",
      "It got right 3 letters of: 1 words.\n",
      "It got right 2 letters of: 16 words.\n",
      "It got right 1 letters of: 82 words.\n",
      "It got right 0 letters of: 168 words.\n",
      "\n",
      "Those are the words and hit count:\n",
      "c7dc    3\n",
      "bxx6    2\n",
      "xmm7    2\n",
      "6dbb    2\n",
      "bn66    2\n",
      "       ..\n",
      "7adc    1\n",
      "mxxx    1\n",
      "7n66    1\n",
      "hbam    1\n",
      "hbd6    1\n",
      "Name: original_file, Length: 99, dtype: int64\n",
      "Those are the letters:\n",
      "c    21\n",
      "x    17\n",
      "d    17\n",
      "7    16\n",
      "6    11\n",
      "n    10\n",
      "b     8\n",
      "h     7\n",
      "m     7\n",
      "a     3\n",
      "Name: label, dtype: int64\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "## Predicting and scoring the test\n",
    "train_predict_lda, test_predict_lda = process_data(training_data, validation_data, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[148   0   1   0   1   0   0   0   0   0]\n",
      " [  1 141   0   0   1   0   1   0   0   0]\n",
      " [  0   0 131   0   4   0   1   0   0   0]\n",
      " [  0   0   1 155   0   0   0   0   0   0]\n",
      " [  0   0   2   0 136   0   1   0   0   0]\n",
      " [  0   0   1   0   0 139   0   0   0   0]\n",
      " [  0   0   1   0   4   0 132   0   0   0]\n",
      " [  0   0   0   0   1   0   0 147   0   0]\n",
      " [  0   0   0   0   1   0   0   0 135   0]\n",
      " [  1   0   0   0   0   0   0   0   0 137]]\n",
      "\n",
      " Other Observations:\n",
      "It got right: 1401 from 1424 letters: 98.38%\n",
      "It received 356 words. \n",
      "It got right 4 letters of: 333 words.\n",
      "It got right 3 letters of: 23 words.\n",
      "It got right 2 letters of: 0 words.\n",
      "It got right 1 letters of: 0 words.\n",
      "It got right 0 letters of: 0 words.\n",
      "\n",
      "Those are the words and hit count:\n",
      "mdn6    4\n",
      "cdma    4\n",
      "d776    4\n",
      "c6dn    4\n",
      "ahbn    4\n",
      "       ..\n",
      "hnxb    3\n",
      "hann    3\n",
      "hhcn    3\n",
      "hhha    3\n",
      "abhm    3\n",
      "Name: original_file, Length: 356, dtype: int64\n",
      "Those are the letters:\n",
      "b    155\n",
      "6    148\n",
      "m    147\n",
      "7    141\n",
      "d    139\n",
      "x    137\n",
      "c    136\n",
      "n    135\n",
      "h    132\n",
      "a    131\n",
      "Name: label, dtype: int64\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## LDA\n",
    "### Scoring the training\n",
    "score_classifier(training_data, train_predict_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 6  6  2 48  2  3  0  6 16 11]\n",
      " [15  7  0 51  1  2  0  7 12 19]\n",
      " [11  5  0 47  1  5  1  8  7 12]\n",
      " [ 5  4  2 61  3  6  0  3 13  9]\n",
      " [11  8  0 59  4  2  1  2  9 11]\n",
      " [ 8  8  1 58  3  4  0  6 14  8]\n",
      " [12  3  0 55  2  5  2 11 16 16]\n",
      " [13  6  4 56  3  0  0  4  5 13]\n",
      " [10  4  2 72  0  1  0  4  9 11]\n",
      " [ 7  4  0 54  1  4  0  5  6 14]]\n",
      "\n",
      " Other Observations:\n",
      "It got right: 111 from 1068 letters: 10.39%\n",
      "It received 267 words. \n",
      "It got right 4 letters of: 0 words.\n",
      "It got right 3 letters of: 1 words.\n",
      "It got right 2 letters of: 11 words.\n",
      "It got right 1 letters of: 86 words.\n",
      "It got right 0 letters of: 169 words.\n",
      "\n",
      "Those are the words and hit count:\n",
      "dbbb    3\n",
      "bxx6    2\n",
      "bnbx    2\n",
      "mxxx    2\n",
      "7bnm    2\n",
      "       ..\n",
      "7bah    1\n",
      "xdbc    1\n",
      "bn66    1\n",
      "b76b    1\n",
      "xbha    1\n",
      "Name: original_file, Length: 98, dtype: int64\n",
      "Those are the letters:\n",
      "b    61\n",
      "x    14\n",
      "n     9\n",
      "7     7\n",
      "6     6\n",
      "d     4\n",
      "c     4\n",
      "m     4\n",
      "h     2\n",
      "Name: label, dtype: int64\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "## NB\n",
    "### Predict and scoring the validation\n",
    "train_predict_nb, test_predict_nb = process_data(training_data, validation_data, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[35  3  6 73  1  2  0  9  6 15]\n",
      " [14 24  1 73  1  2  2  4  8 15]\n",
      " [12  6 15 79  0  2  0  4  9  9]\n",
      " [14 10  2 97  2  0  0  2 10 19]\n",
      " [ 7 11  1 72 15  2  0  7  8 16]\n",
      " [13  8  1 74  1 17  0  2 13 11]\n",
      " [13  2  1 76  3  1 13 10  6 12]\n",
      " [13  5  3 72  0  2  0 25  9 19]\n",
      " [12  8  0 73  2  2  0  2 21 16]\n",
      " [14  6  2 65  0  2  0  5  8 36]]\n",
      "\n",
      " Other Observations:\n",
      "It got right: 298 from 1424 letters: 20.93%\n",
      "It received 356 words. \n",
      "It got right 4 letters of: 0 words.\n",
      "It got right 3 letters of: 13 words.\n",
      "It got right 2 letters of: 59 words.\n",
      "It got right 1 letters of: 141 words.\n",
      "It got right 0 letters of: 143 words.\n",
      "\n",
      "Those are the words and hit count:\n",
      "6ndn    3\n",
      "bmdb    3\n",
      "7bbb    3\n",
      "abx7    3\n",
      "mnbc    3\n",
      "       ..\n",
      "nam6    1\n",
      "7h6h    1\n",
      "bx66    1\n",
      "6h7d    1\n",
      "7dmb    1\n",
      "Name: original_file, Length: 213, dtype: int64\n",
      "Those are the letters:\n",
      "b    97\n",
      "x    36\n",
      "6    35\n",
      "m    25\n",
      "7    24\n",
      "n    21\n",
      "d    17\n",
      "a    15\n",
      "c    15\n",
      "h    13\n",
      "Name: label, dtype: int64\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Scoring the training data\n",
    "score_classifier(training_data, train_predict_nb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
